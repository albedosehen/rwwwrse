# Fluentd Configuration for rwwwrse Production Logging

# System configuration
<system>
  workers 2
  root_dir /fluentd/log
</system>

# Input sources
<source>
  @type forward
  @id input_forward
  bind 0.0.0.0
  port 24224
  @log_level info
</source>

# Docker container logs
<source>
  @type tail
  @id input_docker_logs
  path /var/lib/docker/containers/*/*.log
  pos_file /fluentd/log/docker.log.pos
  tag docker.*
  format json
  read_from_head true
</source>

# rwwwrse application logs
<source>
  @type tail
  @id input_rwwwrse_logs
  path /var/log/rwwwrse/*.log
  pos_file /fluentd/log/rwwwrse.log.pos
  tag rwwwrse.*
  format json
  read_from_head true
</source>

# Nginx access logs
<source>
  @type tail
  @id input_nginx_access
  path /var/log/nginx/access.log
  pos_file /fluentd/log/nginx_access.log.pos
  tag nginx.access
  format nginx
  read_from_head true
</source>

# Nginx error logs
<source>
  @type tail
  @id input_nginx_error
  path /var/log/nginx/error.log
  pos_file /fluentd/log/nginx_error.log.pos
  tag nginx.error
  format /^(?<time>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?<level>\w+)\] (?<pid>\d+).(?<tid>\d+): (?<message>.*)$/
  time_format %Y/%m/%d %H:%M:%S
  read_from_head true
</source>

# PostgreSQL logs
<source>
  @type tail
  @id input_postgres_logs
  path /var/log/postgresql/*.log
  pos_file /fluentd/log/postgres.log.pos
  tag postgres.*
  format /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3} \w+) \[(?<pid>\d+)\] (?<level>\w+):  (?<message>.*)$/
  time_format %Y-%m-%d %H:%M:%S.%L %Z
  read_from_head true
</source>

# Redis logs
<source>
  @type tail
  @id input_redis_logs
  path /var/log/redis/*.log
  pos_file /fluentd/log/redis.log.pos
  tag redis.*
  format /^(?<pid>\d+):(?<role>\w) (?<time>\d{2} \w{3} \d{4} \d{2}:\d{2}:\d{2}\.\d{3}) (?<level>[*#-]) (?<message>.*)$/
  time_format %d %b %Y %H:%M:%S.%L
  read_from_head true
</source>

# Filter to parse Docker container logs
<filter docker.**>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  <parse>
    @type json
  </parse>
</filter>

# Add Docker metadata
<filter docker.**>
  @type docker_metadata
  docker_url unix:///var/run/docker.sock
  container_id_regexp (?<container_id>[a-z0-9]{64})
  container_name_regexp (?<container_name>[^_]+)
</filter>

# Filter for rwwwrse logs - add structured fields
<filter rwwwrse.**>
  @type record_transformer
  <record>
    service rwwwrse
    environment production
    log_type application
  </record>
</filter>

# Filter for nginx logs - enhance with geo data
<filter nginx.access>
  @type record_transformer
  <record>
    service nginx
    environment production
    log_type access
  </record>
</filter>

<filter nginx.error>
  @type record_transformer
  <record>
    service nginx
    environment production
    log_type error
  </record>
</filter>

# Filter for database logs
<filter postgres.**>
  @type record_transformer
  <record>
    service postgres
    environment production
    log_type database
  </record>
</filter>

<filter redis.**>
  @type record_transformer
  <record>
    service redis
    environment production
    log_type cache
  </record>
</filter>

# Security log filtering - detect suspicious patterns
<filter **>
  @type grep
  <regexp>
    key message
    pattern /(SQL injection|XSS|CSRF|brute force|unauthorized|malicious)/i
  </regexp>
  <inject>
    tag security.alert
  </inject>
</filter>

# Error log filtering
<filter **>
  @type grep
  <regexp>
    key level
    pattern /(ERROR|CRITICAL|FATAL)/i
  </regexp>
  <inject>
    tag error.alert
  </inject>
</filter>

# Performance monitoring - slow requests
<filter nginx.access>
  @type numeric_monitor
  tag slow_request
  monitor_key request_time
  <record>
    type slow_request
    threshold 2.0
  </record>
</filter>

# Output to Elasticsearch
<match **>
  @type elasticsearch
  @id output_elasticsearch
  host elasticsearch
  port 9200
  index_name fluentd-${tag}-%Y%m%d
  type_name _doc
  
  # Performance settings
  bulk_message_request_threshold 1048576
  bulk_message_flush_interval 10s
  chunk_limit_size 256m
  queue_limit_length 512
  retry_max_interval 30s
  retry_forever true
  
  # Template for index mapping
  template_name rwwwrse
  template_file /fluentd/conf/elasticsearch_template.json
  template_overwrite true
  
  # Time-based indexing
  logstash_format true
  logstash_prefix rwwwrse
  logstash_dateformat %Y%m%d
  
  # Buffer configuration
  <buffer tag,time>
    @type file
    path /fluentd/log/buffer
    timekey 3600
    timekey_wait 10m
    timekey_use_utc true
    chunk_limit_size 256m
    total_limit_size 2GB
    flush_mode interval
    flush_interval 30s
    flush_thread_count 2
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_times 3
    overflow_action drop_oldest_chunk
  </buffer>
</match>

# Security alerts to separate index
<match security.alert>
  @type elasticsearch
  @id output_security_elasticsearch
  host elasticsearch
  port 9200
  index_name security-alerts-%Y%m%d
  type_name _doc
  
  <buffer tag,time>
    @type file
    path /fluentd/log/security_buffer
    timekey 300
    timekey_wait 30s
    flush_mode immediate
  </buffer>
</match>

# Error alerts to separate index
<match error.alert>
  @type elasticsearch
  @id output_error_elasticsearch
  host elasticsearch
  port 9200
  index_name error-alerts-%Y%m%d
  type_name _doc
  
  <buffer tag,time>
    @type file
    path /fluentd/log/error_buffer
    timekey 300
    timekey_wait 30s
    flush_mode immediate
  </buffer>
</match>

# Slow request monitoring
<match slow_request>
  @type elasticsearch
  @id output_slow_request_elasticsearch
  host elasticsearch
  port 9200
  index_name performance-alerts-%Y%m%d
  type_name _doc
  
  <buffer tag,time>
    @type file
    path /fluentd/log/performance_buffer
    timekey 300
    timekey_wait 30s
    flush_mode immediate
  </buffer>
</match>

# Backup to S3 (optional)
<match **>
  @type copy
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
    s3_bucket "#{ENV['S3_BACKUP_BUCKET']}"
    s3_region "#{ENV['AWS_REGION']}"
    path logs/
    time_slice_format %Y%m%d%H
    buffer_chunk_limit 256m
    
    <buffer time>
      @type file
      path /fluentd/log/s3_buffer
      timekey 3600
      timekey_wait 10m
      chunk_limit_size 256m
      total_limit_size 1GB
    </buffer>
  </store>
</match>

# Debug output (disable in production)
# <match **>
#   @type stdout
#   @id output_stdout
# </match>